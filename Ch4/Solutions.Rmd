---
title: "Solutions to Applied Questions — Chapter 4"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=F}
library(ISLR2)
library(ISLR)
library(dplyr)
library(ggplot2)
```

# Questions 13

This question should be answered using the `Weekly` data set, which is part of the `ISLR2` package. This data is similar in nature to the `Smarket` data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.

**a) Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?**

There appears to be an exponential relation between `Year` and `Volume`. There is also a significance in the correlation between all the lags. `Volume` appears significantly correlated with the other variables as well, except possibly `Today` which is only significantly correlated with `Lag1`, `Lag2`, and `Lag3`.

```{r}
summary(Weekly)
Hmisc::rcorr(as.matrix(Weekly[,-9]))
pairs(Weekly, col=Weekly$Direction)
```


**(b) Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?**

`Lag2` is the only significant predictor in this model.

```{r}
model <- glm(Direction ~ . - Year - Today, data = Weekly, family = "binomial")
summary(model)
```

**(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.**

It is only very slightly better than a random walk. However, in the field of finance, any margins a vital.

```{r}
probs <- predict(model, type = "response")
preds <- rep("Down", length(probs))
preds[probs > .5] = "Up"
table(preds, Weekly$Direction)
mean(preds == Weekly$Direction)
```


**(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with `Lag2` as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).**

We can observe an improvement from .56 to .65 in the prediction error rate.

```{r}
model <- glm(
    Direction ~ Lag2,
    data = Weekly,
    family = "binomial",
    subset = Year <= 2008
)

subData <- subset(Weekly, Weekly$Year > 2008)
probs <- predict(model,
                 newdata = subData,
                 type = "response")

preds <- rep("Down", length(probs))
preds[probs > .5] = "Up"
table(preds, subData$Direction)
mean(preds == subData$Direction)
```

**(e) Repeat (d) using LDA.**

Performce slightly worse than logistic regression.

```{r}
model <- MASS::lda(Direction ~ Lag2,
          data = Weekly,
          subset = Year <= 2008)

preds <- predict(model,
                 newdata = subData,
                 type = "response")

table(preds$class, subData$Direction)
mean(preds$class == subData$Direction)
```

**(f) Repeat (d) using QDA.**

Both worse performing than LDA and logistic.

```{r}
model <- MASS::qda(Direction ~ Lag2,
          data = Weekly,
          subset = Year <= 2008)

preds <- predict(model,
                 newdata = subData,
                 type = "response")

table(preds$class, subData$Direction)
mean(preds$class == subData$Direction)
```

**(g) Repeat (d) using KNN with K = 1.**

Like a random walk.

```{r}
preds <-
    class::knn(train = Weekly[Weekly$Year < 2009, "Lag2", drop = FALSE],
               test = Weekly[!(Weekly$Year < 2009), "Lag2", drop = FALSE],
               cl = Weekly$Direction[Weekly$Year < 2009])
table(preds, subData$Direction)
mean(preds == subData$Direction)
```


**(h) Repeat (d) using naive Bayes.**

Same performance as QDA.

```{r}
model <- e1071::naiveBayes(Direction ~ Lag2,
          data = Weekly,
          subset = Year <= 2008)

preds <- predict(model,
                 newdata = subData)

table(preds, subData$Direction)
mean(preds == subData$Direction)
```

**(i) Which of these methods appears to provide the best results on this data?**

Logistics.

# Queestion 14. 

```{r}

```

In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the `Auto` data set.

**(a) Create a binary variable, `mpg01`, that contains a 1 if `mpg` contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both `mpg01` and the other Auto variables.**

```{r}
mpg01 <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
df <- Auto %>% cbind(mpg01) %>% select(-name)
```


**(b) Explore the data graphically in order to investigate the association between `mpg01` and the other features. Which of the other features seem most likely to be useful in predicting `mpg01`? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.**

No NA's. Everything has a significant correlation to each other. `mpg` appears postively linear with `origin`, `acceleration`, and `year`; negatively linear with `cylinders`; and exponentially negative with `displacement`, `horsepower`, and `weight`.

```{r}
naniar::vis_miss(df)
Hmisc::rcorr(as.matrix(df[,-9:-10]))
pairs(df[,-9:-10])
```

There is appearing to be a non linear relationship with the number of cylinders and  `mpg` is higher on average for Japanese than European, and European than American.

```{r}
df <- df %>% mutate(cylinders = as.factor(cylinders), origin = as.factor(origin))
levels(df$origin) <- c("American", "European", "Japanese")

a <- df %>% ggplot() +
    geom_boxplot(aes(y = mpg, x = cylinders))
b <- df %>% ggplot() +
    geom_boxplot(aes(y = mpg, x = origin))

ggpubr::ggarrange(a, b, ncol = 2, nrow = 1)
```

By using a linear regression as temporary model for diagnostics, we observe that both predictors are significant.

```{r}
summary(lm(mpg ~ cylinders + origin + year, data = df))
```


we see very strong relationship between `displacement`, `horsepower`, and `weight`, however with similar shape—might indicate high VIF. There appears to be a weaker realtionship with `accelartion` and `year.`

```{r}
a <- df %>% ggplot() +
    geom_point(aes(y = mpg, x = displacement))

b <- df %>% ggplot() +
    geom_point(aes(y = mpg, x = horsepower))

c <- df %>% ggplot() +
    geom_point(aes(y = mpg, x = weight))

d <- df %>% ggplot() +
    geom_point(aes(y = mpg, x = acceleration))

e <- df %>% ggplot() +
    geom_point(aes(y = mpg, x = year))

ggpubr::ggarrange(a, b, c, d, e, ncol = 3, nrow = 2)
```

Indeed, there is very high correlation between the three variables.

```{r}
df %>% select(displacement, horsepower, weight) %>% cor()
```

We observe in a temporary model, that removing displacement significantly reduce VIF for each variable.

```{r}
car::vif(lm(mpg~displacement + horsepower + weight, data = df))
car::vif(lm(mpg~horsepower + weight, data = df))
```

However, including the other variables deem `horsepower` and `accelaration` insignificant. Indeed, only `weight` and `year` had significance.

```{r}
summary(lm(mpg ~ horsepower + weight + acceleration + year,
           data = df))$coefficients
summary(lm(mpg ~ weight + acceleration + year, data = df))$coefficients
summary(lm(mpg ~ weight + year, data = df))$coefficients
```

Including every predictor, we see significance across. Including an interaction term between weight and and year produces a significant result as well.

```{r}
summary(lm(mpg ~ weight + year + cylinders + origin, data = df))$coef
summary(lm(mpg ~ weight*year + cylinders + origin, data = df))$coef
```



**(c) Split the data into a training set and a test set.**

We use 60% of dataset as training set and 40% as test set.

```{r}
df$id <- 1:nrow(df)
train <- df %>% sample_frac(0.60)
test  <- anti_join(df, train, by = 'id')
```


**(d) Perform LDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?**


```{r}
model_lda <- MASS::lda(mpg01 ~ weight*year + cylinders + origin, data = train)
preds_lda <- predict(model_lda, newdata = test)$class
mean(preds_lda == test$mpg01)
```


**(e) Perform QDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?**

```{r}
model_qda <- MASS::qda(mpg01 ~ weight*year + cylinders + origin, data = train)
preds_qda <- predict(model_qda, newdata = test)$class
mean(preds_qda == test$mpg01)
```

**(f) Perform logistic regression on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?**

```{r}
model_log <-
    glm(mpg01 ~ weight * year + cylinders + origin,
        data = train,
        family = "binomial")
probs_log <- predict(model_log, newdata = test)
preds_log <- ifelse(probs_log > .5, 1, 0)
mean(preds_log == test$mpg01)
```

**(g) Perform naive Bayes on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?**

```{r}
model_naive <-
    e1071::naiveBayes(mpg01 ~ weight + year + cylinders + origin, data = train)
preds_naive <- predict(model_naive, newdata = test)
mean(preds_naive == test$mpg01)
```


**(h) Perform KNN on the training data, with several values of K, in order to predict `mpg01.` Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?**

k = 5 minimizes test error.

```{r}
model_knn <- caret::knn3(mpg01 ~ weight * year + cylinders + origin,
                         data = train,
                         k = 5)

probs_knn <- predict(model_knn, newdata = test)
preds_knn <- ifelse(probs_knn[,2] > .5, 1, 0)
mean(preds_knn == test$mpg01)
```

# Question 15

This problem involves writing functions.

**(a) Write a function, `Power()`, that prints out the result of raising 2 to the 3rd power. In other words, your function should compute 23 and print out the results.**

*Hint: Recall that `x^a` raises `x` to the power `a`. Use the `print()` function to output the result.*

```{r}
Power <- function() {
    print(2^3)
}
```


**(b) Create a new function, `Power2()`, that allows you to pass any two numbers, `x` and `a`, and prints out the value of `x^a`. You can do this by beginning your function with the line `>Power2 <- function(x, a) {` You should be able to call your function by entering, for instance, `> Power2(3, 8)` on the command line. This should output the value of 38, namely, 6, 561.**

```{r}
Power2 <- function(x, a) {
    print(x^a)
}

Power2(3, 8)
```


**(c) Using the `Power2()` function that you just wrote, compute 103, 817, and 1313.**

```{r}
Power2(10, 3)
Power2(8, 17)
Power2(131, 3)
```

**(d) Now create a new function, `Power3()`, that actually returns the result `x^a` as an R object, rather than simply printing it to the screen. That is, if you store the value `x^a` in an object called result within your function, then you can simply `return()` this result, using the following line: `return(result)` The line above should be the last line in your function, before the `}` symbol.**

```{r}
Power3 <- function(x, a) {
    result <- x^a
    return(result)
}
```


**(e) Now using the `Power3()` function, create a plot of `f(x) = x2`. The x-axis should display a range of integers from 1 to 10, and the y-axis should display `x2`. Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using `log = "x"`, `log = "y"`, or `log = "xy"` as arguments to the `plot()` function.**

```{r}
x_range <- seq(from = 1, to = 10, by = 1)
y_range <- sapply(x_range, Power3, 2)

ggplot(map = aes(x = x_range, y = y_range)) +
    geom_line() +
    ylab("x^2") +
    xlab("x")
```


**(f) Create a function, `PlotPower()`, that allows you to create a plot of `x` against `x^a` for a fixed `a` and for a range of values of `x`. For instance, if you call `> PlotPower(1:10, 3)` then a plot should be created with an x-axis taking on values 1,2,...,10, and a y-axis taking on values 13,23,...,103.**

```{r}
PlotPower <- function(x, a) {
    ggplot(map = aes(x = x, y = Power3(x, a))) +
        geom_line() +
        ylab(paste("x^", a, sep = "")) +
        xlab("x")
}
PlotPower(1:10, 3)
```

